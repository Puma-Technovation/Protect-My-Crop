{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPAvQYe2PfKF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv('data.csv')\n",
        "# Drop unnecessary columns (Mes and Unnamed: 0)\n",
        "\n",
        "# Check for NaN or infinite values and replace/remove them\n",
        "data = data.replace([np.inf, -np.inf], np.nan)\n",
        "data = data.dropna()  # Remove rows with NaN values\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=['Target'])\n",
        "y = data['Target']\n",
        "\n",
        "# Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data (scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build a neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))  # Input layer\n",
        "model.add(Dense(32, activation='relu'))  # Hidden layer\n",
        "model.add(Dense(1, activation='linear'))  # Output layer for regression\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping to prevent overfitting\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=10, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "train_predictions = model.predict(X_train_scaled)\n",
        "test_predictions = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate Mean Squared Error for both train and test sets\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Mean Squared Error)')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print final MSE values\n",
        "print(f\"Final Train Mean Squared Error: {train_mse}\")\n",
        "print(f\"Final Test Mean Squared Error: {test_mse}\")\n",
        "import pickle\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save('model.h5')\n",
        "\n",
        "# Guardar el scaler\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ]
    }
  ]
}